{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from helpers.helper_functions import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handling text 2 exercise\n",
    "[Handling text exercisses ADApted drom ADA 2018 final exam]\n",
    "\n",
    "The Sheldon Cooper we all know and love (OK, some of us might not know him, and some might not love him) from the TV series \"The Big Bang Theory\" has gotten into an argument with Leonard from the same TV show. Sheldon insists that he knows the show better than anyone, and keeps making various claims about the show, which neither of them know how to prove or disprove. The two of them have reached out to you ladies and gentlemen, as data scientists, to help them. You will be given the full script of the series, with information on the episode, the scene, the person saying each dialogue line, and the dialogue lines themselves.\n",
    "\n",
    "Leonard has challenged several of Sheldon's claims about the show, and throughout this exam you will see some of those and you will get to prove or disprove them, but remember: sometimes, we can neither prove a claim, nor disprove it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task A: Picking up the shovel\n",
    "\n",
    "**Note: You will use the data you preprocess in this task in all the subsequent ones.**\n",
    "\n",
    "Our friends' argument concerns the entire show. We have given you a file in the `data/` folder that contains the script of every single episode. New episodes are indicated by '>>', new scenes by '>', and the rest of the lines are dialogue lines. Some lines are said by multiple people (for example, lines indicated by 'All' or 'Together'); **you must discard these lines**, for the sake of simplicity. However, you do not need to do it for Q1 in this task -- you'll take care of it when you solve Q2.\n",
    "\n",
    "**Q1**. Your first task is to extract all lines of dialogue in each scene and episode, creating a dataframe where each row has the episode and scene where a dialogue line was said, the character who said it, and the line itself. You do not need to extract the proper name of the episode (e.g. episode 1 can appear as \"Series 01 Episode 01 - Pilot Episode\", and doesn't need to appear as \"Pilot Episode\"). Then, answer the following question: In total, how many scenes are there in each season? We're not asking about unique scenes; the same location appearing in two episodes counts as two scenes. You can use a Pandas dataframe with a season column and a scene count column as the response.\n",
    "\n",
    "**Note: The data refers to seasons as \"series\".**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['>> Series 01 Episode 01 – Pilot\\xa0Episode',\n",
       " '> A corridor at a sperm bank.',\n",
       " 'Sheldon: So if a photon is directed through a plane with two slits in it and either slit is observed it will not go through both slits. If it’s unobserved it will, however, if it’s observed after it’s left the plane but before it hits its target, it will not have gone through both slits.']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import codecs\n",
    "file_path = 'data/all_scripts.txt'\n",
    "with codecs.open(file_path, 'r', 'utf-8') as f:\n",
    "    raw = f.read().strip('\\n').split('\\n')\n",
    "    \n",
    "raw[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**QUESTION: bad encoding for the `\\xa0` character (1st line)?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first parse the raw text file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Series 01 Episode 01 – Pilot Episode\n",
      "A corridor at a sperm bank.\n"
     ]
    }
   ],
   "source": [
    "parse_episode = lambda s: s[3:]\n",
    "parse_scene = lambda s: s[2:]\n",
    "\n",
    "print(parse_episode(raw[0]))\n",
    "print(parse_scene(raw[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Sheldon',\n",
       " 'So if a photon is directed through a plane with two slits in it and either slit is observed it will not go through both slits. If it’s unobserved it will, however, if it’s observed after it’s left the plane but before it hits its target, it will not have gone through both slits.')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def parse_dialog_line(line):\n",
    "    i = line.find(':')\n",
    "    return line[:i], line[i+2:] # +2 to skip : as well as the following space\n",
    "\n",
    "parse_dialog_line(raw[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of tuples: (episode, scene, character, dialog line)\n",
    "rows = []\n",
    "\n",
    "cur_episode, cur_scene = parse_episode(raw[0]), parse_scene(raw[1])\n",
    "for line in raw[2:]:\n",
    "    if line.startswith('>>'): # new episode\n",
    "        cur_episode = parse_episode(line)\n",
    "        cur_scene = np.nan\n",
    "    elif line.startswith('>'): # new scene\n",
    "        cur_scene = parse_scene(line)\n",
    "    else: # simple line\n",
    "        character, dialog = parse_dialog_line(line)\n",
    "        rows.append((\n",
    "            cur_episode, cur_scene, character, dialog\n",
    "        ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And convert it to a DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>episode</th>\n",
       "      <th>scene</th>\n",
       "      <th>character</th>\n",
       "      <th>line</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Series 01 Episode 01 – Pilot Episode</td>\n",
       "      <td>A corridor at a sperm bank.</td>\n",
       "      <td>Sheldon</td>\n",
       "      <td>So if a photon is directed through a plane wit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Series 01 Episode 01 – Pilot Episode</td>\n",
       "      <td>A corridor at a sperm bank.</td>\n",
       "      <td>Leonard</td>\n",
       "      <td>Agreed, what’s your point?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Series 01 Episode 01 – Pilot Episode</td>\n",
       "      <td>A corridor at a sperm bank.</td>\n",
       "      <td>Sheldon</td>\n",
       "      <td>There’s no point, I just think it’s a good ide...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Series 01 Episode 01 – Pilot Episode</td>\n",
       "      <td>A corridor at a sperm bank.</td>\n",
       "      <td>Leonard</td>\n",
       "      <td>Excuse me?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Series 01 Episode 01 – Pilot Episode</td>\n",
       "      <td>A corridor at a sperm bank.</td>\n",
       "      <td>Receptionist</td>\n",
       "      <td>Hang on.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51287</th>\n",
       "      <td>Series 10 Episode 24 – The Long Distance Disso...</td>\n",
       "      <td>Sheldon’s office.</td>\n",
       "      <td>Ramona</td>\n",
       "      <td>Mmm. No big deal, I enjoy spending time with you.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51288</th>\n",
       "      <td>Series 10 Episode 24 – The Long Distance Disso...</td>\n",
       "      <td>Sheldon’s office.</td>\n",
       "      <td>Sheldon</td>\n",
       "      <td>And I with you. Question, are you seeking a ro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51289</th>\n",
       "      <td>Series 10 Episode 24 – The Long Distance Disso...</td>\n",
       "      <td>Sheldon’s office.</td>\n",
       "      <td>Ramona</td>\n",
       "      <td>What if I were?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51290</th>\n",
       "      <td>Series 10 Episode 24 – The Long Distance Disso...</td>\n",
       "      <td>Sheldon’s office.</td>\n",
       "      <td>Sheldon</td>\n",
       "      <td>Well, that would raise a number of problems. W...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51291</th>\n",
       "      <td>Series 10 Episode 24 – The Long Distance Disso...</td>\n",
       "      <td>Princeton.</td>\n",
       "      <td>Sheldon</td>\n",
       "      <td>(Knock, knock, knock) Amy. (Knock, knock, knoc...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>51292 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 episode  \\\n",
       "0                   Series 01 Episode 01 – Pilot Episode   \n",
       "1                   Series 01 Episode 01 – Pilot Episode   \n",
       "2                   Series 01 Episode 01 – Pilot Episode   \n",
       "3                   Series 01 Episode 01 – Pilot Episode   \n",
       "4                   Series 01 Episode 01 – Pilot Episode   \n",
       "...                                                  ...   \n",
       "51287  Series 10 Episode 24 – The Long Distance Disso...   \n",
       "51288  Series 10 Episode 24 – The Long Distance Disso...   \n",
       "51289  Series 10 Episode 24 – The Long Distance Disso...   \n",
       "51290  Series 10 Episode 24 – The Long Distance Disso...   \n",
       "51291  Series 10 Episode 24 – The Long Distance Disso...   \n",
       "\n",
       "                             scene     character  \\\n",
       "0      A corridor at a sperm bank.       Sheldon   \n",
       "1      A corridor at a sperm bank.       Leonard   \n",
       "2      A corridor at a sperm bank.       Sheldon   \n",
       "3      A corridor at a sperm bank.       Leonard   \n",
       "4      A corridor at a sperm bank.  Receptionist   \n",
       "...                            ...           ...   \n",
       "51287            Sheldon’s office.        Ramona   \n",
       "51288            Sheldon’s office.       Sheldon   \n",
       "51289            Sheldon’s office.        Ramona   \n",
       "51290            Sheldon’s office.       Sheldon   \n",
       "51291                   Princeton.       Sheldon   \n",
       "\n",
       "                                                    line  \n",
       "0      So if a photon is directed through a plane wit...  \n",
       "1                             Agreed, what’s your point?  \n",
       "2      There’s no point, I just think it’s a good ide...  \n",
       "3                                             Excuse me?  \n",
       "4                                               Hang on.  \n",
       "...                                                  ...  \n",
       "51287  Mmm. No big deal, I enjoy spending time with you.  \n",
       "51288  And I with you. Question, are you seeking a ro...  \n",
       "51289                                    What if I were?  \n",
       "51290  Well, that would raise a number of problems. W...  \n",
       "51291  (Knock, knock, knock) Amy. (Knock, knock, knoc...  \n",
       "\n",
       "[51292 rows x 4 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(rows, columns=['episode', 'scene', 'character', 'line'])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first count the number of unique scenes by episode:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "episode\n",
       "Series 01 Episode 01 – Pilot Episode                   12\n",
       "Series 01 Episode 02 – The Big Bran Hypothesis         11\n",
       "Series 01 Episode 03 – The Fuzzy Boots Corollary       11\n",
       "Series 01 Episode 04 – The Luminous Fish Effect        14\n",
       "Series 01 Episode 05 – The Hamburger Postulate          8\n",
       "                                                       ..\n",
       "Series 10 Episode 20 – The Recollection Dissipation     9\n",
       "Series 10 Episode 21 – The Separation Agitation         7\n",
       "Series 10 Episode 22 – The Cognition Regeneration      10\n",
       "Series 10 Episode 23 – The Gyroscopic Collapse          9\n",
       "Series 10 Episode 24 – The Long Distance Dissonance     9\n",
       "Name: scene, Length: 231, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby('episode').scene.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then sum those unique scenes per episode to a count of scenes per season:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n_scenes</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>season</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>01</th>\n",
       "      <td>157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>02</th>\n",
       "      <td>205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>03</th>\n",
       "      <td>193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>04</th>\n",
       "      <td>219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>05</th>\n",
       "      <td>198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>06</th>\n",
       "      <td>211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>07</th>\n",
       "      <td>207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>08</th>\n",
       "      <td>191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>09</th>\n",
       "      <td>177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>189</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        n_scenes\n",
       "season          \n",
       "01           157\n",
       "02           205\n",
       "03           193\n",
       "04           219\n",
       "05           198\n",
       "06           211\n",
       "07           207\n",
       "08           191\n",
       "09           177\n",
       "10           189"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_scenes = pd.DataFrame(df.groupby('episode').scene.nunique()).reset_index()\n",
    "# Add column 'season'\n",
    "df_scenes['season'] = df_scenes.episode.apply(lambda s: s.split(' ')[1])\n",
    "\n",
    "# Group by season and count scenes\n",
    "df_scenes = pd.DataFrame(df_scenes.groupby('season').scene.sum())\n",
    "df_scenes.columns = ['n_scenes']\n",
    "\n",
    "df_scenes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q2**. Now, let's define two sets of characters: all the characters, and recurrent characters. Recurrent characters are those who appear in more than one episode. For the subsequent sections, you will need to have a list of recurrent characters. Assume that there are no two _named characters_ (i.e. characters who have actual names and aren't referred to generically as \"little girl\", \"grumpy grandpa\", etc.) with the same name, i.e. there are no two Sheldons, etc. Generate a list of recurrent characters who have more than 90 dialogue lines in total, and then take a look at the list you have. If you've done this correctly, you should have a list of 20 names. However, one of these is clearly not a recurrent character. Manually remove that one, and print out your list of recurrent characters. To remove that character, pay attention to the _named character_ assumption we gave you earlier on. **For all the subsequent questions, you must only keep the dialogue lines said by the recurrent characters in your list.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We recall that recurrent characters are the ones who appear in more than one episode, so we group by character and count the episodes (this gives a count of the number of episode per character):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "character\n",
       "Abby                  1\n",
       "Actress               1\n",
       "Adam                  2\n",
       "Air Force Officer     1\n",
       "Alex                  4\n",
       "                     ..\n",
       "Woman                10\n",
       "Woman on TV           2\n",
       "Wyatt                 2\n",
       "Zack                  8\n",
       "Zombie                1\n",
       "Name: episode, Length: 198, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char_episode_count = df.groupby('character').episode.nunique()\n",
    "char_episode_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We filter out characters having a single episode:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Adam', 'Alex', 'Alfred', 'All', 'Amy', 'Arthur', 'Assistant', 'Barman', 'Barry', 'Bernadette', 'Bert', 'Beverley', 'Beverly', 'Both', 'Claire', 'Col. Williams', 'Colonel Williams', 'Dan', 'Dave', 'Dimitri', 'Doctor', 'Emily', 'Everyone', 'Female Voice', 'Gablehauser', 'Gablehouser', 'Girl', 'Girls', 'Guy', 'Howard', 'Howard and Bernadette', 'Howard and Raj', 'Howard’s Mother', 'Ira', 'Janine', 'Katee Sackhoff', 'Kevin', 'Kripke', 'Kurt', 'LeVar', 'Leonard', 'Leonard and Howard', 'Lesley', 'Leslie', 'Lucy', 'Man', 'Man on TV', 'Mary', 'Mike', 'Mr Rostenkowski', 'Mr. Rostenkowski', 'Mrs Cooper', 'Mrs Davis', 'Mrs Hofstadter', 'Mrs Koothrappali', 'Mrs Wolowitz', 'Nurse', 'Penny', 'Penny’s Dad', 'Policeman', 'Priya', 'Raj', 'Ramona', 'Receptionist', 'Santa', 'Security Guard', 'Seibert', 'Sheldon', 'Siri', 'Spock', 'Steph', 'Stephen Hawking', 'Stuart', 'Summer', 'Together', 'Voice', 'Waiter', 'Waitress', 'Wil', 'Wil Wheaton', 'Woman', 'Woman on TV', 'Wyatt', 'Zack']\n"
     ]
    }
   ],
   "source": [
    "recurrent_characters = char_episode_count > 1\n",
    "recurrent_characters = char_episode_count[recurrent_characters].index.tolist()\n",
    "print(recurrent_characters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We count the number of lines per character:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "character\n",
       "Abby                   4\n",
       "Actress                5\n",
       "Adam                  19\n",
       "Air Force Officer      6\n",
       "Alex                  63\n",
       "                    ... \n",
       "Woman                 25\n",
       "Woman on TV            5\n",
       "Wyatt                 42\n",
       "Zack                 135\n",
       "Zombie                 6\n",
       "Name: line, Length: 198, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char_lines = df.groupby('character').line.count()\n",
    "char_lines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We filter out characters having less than 90 dialogue lines by taking the intersection between characters having more than 90 lines and recurrent characters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Amy', 'Arthur', 'Bernadette', 'Bert', 'Beverley', 'Emily', 'Howard', 'Kripke', 'Leonard', 'Leslie', 'Man', 'Mrs Cooper', 'Mrs Wolowitz', 'Penny', 'Priya', 'Raj', 'Sheldon', 'Stuart', 'Wil', 'Zack']\n"
     ]
    }
   ],
   "source": [
    "char_more_90_lines = char_lines > 90\n",
    "char_more_90_lines = char_more_90_lines[char_more_90_lines].index.tolist()\n",
    "print(char_more_90_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['Amy', 'Arthur', 'Bernadette', 'Bert', 'Beverley', 'Emily',\n",
       "       'Howard', 'Kripke', 'Leonard', 'Leslie', 'Man', 'Mrs Cooper',\n",
       "       'Mrs Wolowitz', 'Penny', 'Priya', 'Raj', 'Sheldon', 'Stuart',\n",
       "       'Wil', 'Zack'], dtype='<U21')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "characters = np.intersect1d(recurrent_characters, char_more_90_lines)\n",
    "print(characters.size)\n",
    "characters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We indeed have 20 characters in the end (although those are the same as characters with > 90 lines). We should **remove the \"character\" `Man`**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Amy',\n",
       " 'Arthur',\n",
       " 'Bernadette',\n",
       " 'Bert',\n",
       " 'Beverley',\n",
       " 'Emily',\n",
       " 'Howard',\n",
       " 'Kripke',\n",
       " 'Leonard',\n",
       " 'Leslie',\n",
       " 'Mrs Cooper',\n",
       " 'Mrs Wolowitz',\n",
       " 'Penny',\n",
       " 'Priya',\n",
       " 'Raj',\n",
       " 'Sheldon',\n",
       " 'Stuart',\n",
       " 'Wil',\n",
       " 'Zack']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_chars = characters.tolist()\n",
    "final_chars.remove('Man')\n",
    "print(len(final_chars))\n",
    "final_chars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We eventually filter the data to **keep lines said by the above characters**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep raw data in case we need it later\n",
    "rawdf = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(48346, 5)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df[df.character.isin(final_chars)]\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sanity check\n",
    "df.character.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have a filtered dataframe (names `df`) containing our 19 recurrent characters with more than 90 lines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task B: Read the scripts carefully\n",
    "\n",
    "### Part 1: Don't put the shovel down just yet\n",
    "\n",
    "**Q3**. From each dialogue line, replace punctuation marks (listed in the EXCLUDE_CHARS variable provided in `helpers/helper_functions.py`) with whitespaces, and lowercase all the text. **Do not remove any stopwords, leave them be for all the questions in this task.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to replace characters in `EXCLUDE_CHARS` to whitespaces, we create a translation table with the helper function `str.maketrans`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "efficienttrans_table = str.maketrans({\n",
    "    exclude_char: ' ' for exclude_char in EXCLUDE_CHARS\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Agreed  what s your point '"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test\n",
    "'Agreed, what’s your point?'.translate(trans_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We process each line with `apply` and perform string operations as asked. As a \"backup\" solution and as a mean of easy comparison, we rename the unprocessed `line` column to `raw_line` and create a new column `line` with the processed text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.rename(columns={'line': 'raw_line'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>raw_line</th>\n",
       "      <th>line</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>So if a photon is directed through a plane wit...</td>\n",
       "      <td>so if a photon is directed through a plane wit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Agreed, what’s your point?</td>\n",
       "      <td>agreed  what s your point</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>There’s no point, I just think it’s a good ide...</td>\n",
       "      <td>there s no point  i just think it s a good ide...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Excuse me?</td>\n",
       "      <td>excuse me</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>One across is Aegean, eight down is Nabakov, t...</td>\n",
       "      <td>one across is aegean  eight down is nabakov  t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51284</th>\n",
       "      <td>Uh, breakfast yes, lunch no. I did have a coug...</td>\n",
       "      <td>uh  breakfast yes  lunch no  i did have a coug...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51286</th>\n",
       "      <td>How thoughtful. Thank you.</td>\n",
       "      <td>how thoughtful  thank you</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51288</th>\n",
       "      <td>And I with you. Question, are you seeking a ro...</td>\n",
       "      <td>and i with you  question  are you seeking a ro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51290</th>\n",
       "      <td>Well, that would raise a number of problems. W...</td>\n",
       "      <td>well  that would raise a number of problems  w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51291</th>\n",
       "      <td>(Knock, knock, knock) Amy. (Knock, knock, knoc...</td>\n",
       "      <td>knock  knock  knock  amy   knock  knock  knoc...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>48346 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                raw_line  \\\n",
       "0      So if a photon is directed through a plane wit...   \n",
       "1                             Agreed, what’s your point?   \n",
       "2      There’s no point, I just think it’s a good ide...   \n",
       "3                                             Excuse me?   \n",
       "5      One across is Aegean, eight down is Nabakov, t...   \n",
       "...                                                  ...   \n",
       "51284  Uh, breakfast yes, lunch no. I did have a coug...   \n",
       "51286                         How thoughtful. Thank you.   \n",
       "51288  And I with you. Question, are you seeking a ro...   \n",
       "51290  Well, that would raise a number of problems. W...   \n",
       "51291  (Knock, knock, knock) Amy. (Knock, knock, knoc...   \n",
       "\n",
       "                                                    line  \n",
       "0      so if a photon is directed through a plane wit...  \n",
       "1                             agreed  what s your point   \n",
       "2      there s no point  i just think it s a good ide...  \n",
       "3                                             excuse me   \n",
       "5      one across is aegean  eight down is nabakov  t...  \n",
       "...                                                  ...  \n",
       "51284  uh  breakfast yes  lunch no  i did have a coug...  \n",
       "51286                         how thoughtful  thank you   \n",
       "51288  and i with you  question  are you seeking a ro...  \n",
       "51290  well  that would raise a number of problems  w...  \n",
       "51291   knock  knock  knock  amy   knock  knock  knoc...  \n",
       "\n",
       "[48346 rows x 2 columns]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['line'] = df.raw_line.apply(lambda s: s.translate(trans_table).lower())\n",
    "df[['raw_line', 'line']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q4**. For each term, calculate its \"corpus frequency\", i.e. its number of occurrences in the entire series. Visualize the distribution of corpus frequency using a histogram. Explain your observations. What are the appropriate x and y scales for this plot?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We assume that \"term\" means whitespace-delimited strings, more commonly referred as to words. To extract words from a given string, we use the function `simple_tokeniser` that is provided. To count words, we'll use the `collections.Counter`, that we will apply on a string of all joined lines (`str.join`). As an implementation note, we could have done:\n",
    "\n",
    "```python\n",
    "df.line.apply(simple_tokeniser).apply(Counter).sum()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But this is way (!!) less efficient in terms of computational time (the bottleneck is the `sum` operation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_lines = ' '.join(df.line)\n",
    "word_counts = Counter(simple_tokeniser(merged_lines))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We transform the word counts into a `pandas.Series`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "so               3187\n",
       "if               2490\n",
       "a               13518\n",
       "photon              4\n",
       "is               5444\n",
       "                ...  \n",
       "skyped              1\n",
       "corresponded        1\n",
       "nickels             1\n",
       "highlighted         1\n",
       "relation…           1\n",
       "Name: count, Length: 21102, dtype: int64"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = pd.Series(word_counts, name='count')\n",
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "words.hist()\n",
    "with sns.axes_style('whitegrid'):\n",
    "    plt.yscale('log')\n",
    "\n",
    "plt.xlabel('Word count')\n",
    "plt.ylabel('Number of words')\n",
    "plt.title('Corpus frequency');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We chose a **logarithmic y axis** since the histogram has the typical shape of word frequencies: the majority of words are present only a few times (the big spike on the left of the histogram), and few words are counted a big number of times (the small isolated peak on the right of the histogram):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2: Talkativity\n",
    "**Q5**. For each of the recurrent characters, calculate their total number of words uttered across all episodes. Based on this, who seems to be the most talkative character?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We group the data by character, and process each character's lines in a similar manner as in Q4 (tokenize, set to lower case) and count the total number of words (no need to first count each word's occurence):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "character\n",
       "Bert              1146\n",
       "Kripke            1246\n",
       "Leslie            1249\n",
       "Zack              1427\n",
       "Arthur            1451\n",
       "Mrs Wolowitz      1459\n",
       "Emily             1571\n",
       "Wil               1678\n",
       "Priya             1940\n",
       "Beverley          2029\n",
       "Mrs Cooper        3389\n",
       "Stuart            7955\n",
       "Bernadette       27726\n",
       "Amy              39933\n",
       "Raj              60099\n",
       "Howard           69505\n",
       "Penny            79270\n",
       "Leonard         102496\n",
       "Sheldon         185388\n",
       "Name: line, dtype: int64"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby('character').line.apply(lambda e: len(simple_tokeniser(' '.join(e)))).sort_values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sheldon seems to be the most talktative character, with 185388 words in total."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task D: The Detective's Hat\n",
    "\n",
    "Sheldon claims that given a dialogue line, he can, with an accuracy of above 70%, say whether it's by himself or by someone else. Leonard contests this claim, since he believes that this claimed accuracy is too high. Leonard also suspects that it's easier for Sheldon to distinguish the lines that _aren't_ his, rather than those that _are_. We want you to put on the (proverbial) detective's hat and to investigate this claim.\n",
    "\n",
    "**Q6**. Divide the set of all dialogue lines into two subsets: the training set, consisting of all the seasons except the last two, and the test set, consisting of the last two seasons. Each of your data points (which is one row of your matrix) is one **dialogue line**. Now, use the scikit-learn class **TfIdfVectorizer** to create TF-IDF representations for the data points in your training and test sets. Note that since you're going to train a machine learning model, everything used in the training needs to be independent of the test set. As a preprocessing step, remove stopwords and words that appear only once from your vocabulary. Use the simple tokenizer provided in `helpers/helper_functions.py` as an input to the TfidfVectorizer class, and use the words provided in `helpers/stopwords.txt` as your stopwords."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**QUESTION & TODO: not clear if we should use processed data (lower case, replaced EXCLUDE_CHARS) or not**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first create a row `season` which will allow us to split the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['season'] = df.episode.apply(lambda s: int(s.split(' ')[1]))\n",
    "df.season.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1, 2, 3, 4, 5, 6, 7, 8]), array([ 9, 10]))"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = df[df.season < 9].copy()\n",
    "df_test = df[df.season >= 9].copy()\n",
    "\n",
    "# Sanity check\n",
    "train.season.unique(), test.season.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data provided to `TfidfVectorizer` is simply a list / an array of documents (i.e. lines in our case):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = df_train.line\n",
    "test = df_test.line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `TfidfVectorizer` class accepts the argument `stop_words`, which will remove any word that appears on the provided list. We can actually add words with a single occurence in this list, eventhough those rare words are not stop words as we typically think of them (in summary, the result is the same and there is no need to create a custom function to remove rare words, that would be reinventing the wheel and it's bad and pretentious)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The rare words must be computed on the **training** set, since the test set must not be inspected... before we want to test our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "so             2433\n",
       "if             1892\n",
       "a             10893\n",
       "photon            4\n",
       "is             4347\n",
       "              ...  \n",
       "glug              2\n",
       "whiner            1\n",
       "sway              1\n",
       "untogether        1\n",
       "slate             1\n",
       "Length: 19508, dtype: int64"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_word_counts = Counter(simple_tokeniser(' '.join(train)))\n",
    "train_word_counts = pd.Series(train_word_counts)\n",
    "train_word_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8507 words with a single occurence in train set\n"
     ]
    }
   ],
   "source": [
    "mask = (train_word_counts == 1)\n",
    "train_words_single_occurence = train_word_counts[mask].index\n",
    "print(f'{len(train_words_single_occurence)} words with a single occurence in train set')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load the stop words provided in `stopwords.txt`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i', 'me', 'my', 'myself']"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('helpers/stopwords.txt', 'r') as f:\n",
    "    stop_words = f.read().strip('\\n').split('\\n')\n",
    "    \n",
    "stop_words[:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And merge the stop words with words having a single occurence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8647 words to exclude\n"
     ]
    }
   ],
   "source": [
    "exclude_words = stop_words + train_words_single_occurence.tolist()\n",
    "print(f'{len(exclude_words)} words to exclude')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now call `TfidfVectorize`: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfid = TfidfVectorizer(tokenizer=simple_tokeniser, stop_words=exclude_words)\n",
    "X = tfid.fit_transform(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtest = tfid.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['“penny',\n",
       " '“release',\n",
       " '“soft',\n",
       " '“the',\n",
       " '“well',\n",
       " '“wesley”',\n",
       " '“what',\n",
       " '“you',\n",
       " '”',\n",
       " '…']"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfid.get_feature_names()[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<38177x10862 sparse matrix of type '<class 'numpy.float64'>'\n",
       " \twith 196876 stored elements in Compressed Sparse Row format>,\n",
       " <10169x10862 sparse matrix of type '<class 'numpy.float64'>'\n",
       " \twith 47269 stored elements in Compressed Sparse Row format>)"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X, Xtest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q7**. Find the set of all words in the training set that are only uttered by Sheldon. Is it possible for Sheldon to identify himself only based on these? Use the test set to assess this possibility, and explain your method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first compute the words that are unique to Sheldon (no one else uttered those) in the **train set**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All words of Sheldon\n",
    "words_sheldon = set(simple_tokeniser(' '.join(df_train[df_train.character == 'Sheldon'].line)))\n",
    "# All words NOT of Shelon\n",
    "words_not_sheldon = set(simple_tokeniser(' '.join(df_train[df_train.character != 'Sheldon'].line)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5269"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# All words that only Sheldon uttered\n",
    "words_only_sheldon = [\n",
    "    word\n",
    "    for word in words_sheldon\n",
    "    if word not in words_not_sheldon\n",
    "]\n",
    "\n",
    "len(words_only_sheldon)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can imagine a very simple classifier: given a line, return 1 if it contains *any* word that is unique to Sheldon, return 0 otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the ground truth target\n",
    "ytest = (df_test.character == 'Sheldon') * 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement this (very) simple classifier\n",
    "def simple_classifier(corpus, word_list):\n",
    "    contains_any_word_in_list = corpus.apply(\n",
    "        lambda line: any(map(lambda word: word in line, word_list))\n",
    "    )\n",
    "    return contains_any_word_in_list * 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat = simple_classifier(test, words_only_sheldon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score, recall_score, precision_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "def report_perf(ytest, yhat):\n",
    "    print('Confusion matrix:')\n",
    "    print(confusion_matrix(ytest, yhat, normalize='all'))\n",
    "    print(f'\\nAccuracy = {accuracy_score(ytest, yhat):.4}')\n",
    "    print(f'Recall: {recall_score(ytest, yhat):.4}')\n",
    "    print(f'Precision = {precision_score(ytest, yhat):.4}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix:\n",
      "[[0.19765955 0.58235815]\n",
      " [0.03402498 0.18595732]]\n",
      "\n",
      "Accuracy = 0.3836\n",
      "Recall: 0.8453\n",
      "Precision = 0.242\n"
     ]
    }
   ],
   "source": [
    "# Check performance\n",
    "report_perf(ytest, yhat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The recall is pretty high, meaning that this simple algorithm is often able to detect Sheldon when we give it a line indeed uttered by Sheldon. However, it has a very bad precision, meaning that among the (+) predictions (i.e. lines uttered by Sheldon), only a few actually belong to Sheldon.\n",
    "\n",
    "In summary, this simple algorithm has a pretty bad performance given its **very high false positive rate**. This is not suprising that our method is **not able to generalize**. Indeed, the rare words have a low probability to be uttered by two different characters, eventhough they do not specifically characterize that character. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q8**. Now, perform singular value decomposition (SVD) on the training TF-IDF matrix, and calculate a **25-dimensional approximation** for both the training and test TF-IDF matrices (you can do this using scikit-learn's **TruncatedSVD** class). Then, train a logistic regression classifier with 10-fold cross-validation (using the scikit-learn **LogisticRegressionCV** class) on the output of the SVD that given a dialogue line, tells you whether it's by Sheldon or by someone else.\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;**i)** Report precision, recall and F1-score for both classes (Sheldon and not-Sheldon), as well as accuracy, of your classifier on the training set and the test set. You need to implement the calculation of the evaluation measures (precision, etc.) yourself -- using the scikit-learn functions for them is not allowed.\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;**ii)** What difference do you observe between the model's scores on the training and test sets? What could you infer from the amount of difference you see? What about the difference between scores on the two classes? Given the performance of your classifier, is Leonard right that the accuracy Sheldon claims is unattainable? What about his suspicions about the lines that Sheldon can and cannot distinguish?\n",
    "    \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;**iii)** List 10 of the most extreme false positives and 10 of the most extreme false negatives, in terms of the probabilities predicted by the logistic regression model. What are common features of false positives? What about the false negatives?\n",
    "    \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;**iv)** What is the most important feature in the model? What are the 5 most important words in this feature? _Hint: Think of the definition of an SVD, and that you did an SVD on the TF-IDF matrix with dialogue lines as rows and words as columns. You have projected the original data points onto a 25-dimensional subspace -- you need to look at the unit vectors you used for the projection._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We perform SVD:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.linear_model import LogisticRegressionCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "svd = TruncatedSVD(n_components=25)\n",
    "svd_train = svd.fit_transform(X)\n",
    "svd_test = svd.transform(Xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg = LogisticRegressionCV(cv=10, random_state=0)\n",
    "\n",
    "ytrain = (df_train.character == 'Sheldon') * 1\n",
    "\n",
    "clf = logreg.fit(svd_train, ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "def confusion_matrix(y, yhat):\n",
    "    tn = (y == 0) & (yhat == 0)\n",
    "    fp = (y == 0) & (yhat == 1)\n",
    "    fn = (y == 1) & (yhat == 0)\n",
    "    tp = (y == 1) & (yhat == 1)\n",
    "    \n",
    "    return tuple(map(sum, [tn, fp, fn, tp]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metrics(y, yhat):\n",
    "    tn, fp, fn, tp = confusion_matrix(y, yhat)\n",
    "    \n",
    "    prec = tp / (tp + fp)\n",
    "    rec =  tp / (tp + fn)\n",
    "    acc =  (tp + tn) / (tn + fp + fn + tp)\n",
    "    f1 = 2 * prec * rec / (prec + rec)\n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'recall': rec,\n",
    "        'precision': prec,\n",
    "        'F1': f1\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "def report(y, yhat):\n",
    "    for k, v in metrics(y, yhat).items():\n",
    "        print(f'{k} = {v:.4}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy = 0.7533\n",
      "recall = 0.01428\n",
      "precision = 0.5745\n",
      "F1 = 0.02787\n"
     ]
    }
   ],
   "source": [
    "yhat = clf.predict(svd_train)\n",
    "report(ytrain, yhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy = 0.7789\n",
      "recall = 0.003129\n",
      "precision = 0.28\n",
      "F1 = 0.006189\n"
     ]
    }
   ],
   "source": [
    "yhat = clf.predict(svd_test)\n",
    "report(ytest, yhat)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
